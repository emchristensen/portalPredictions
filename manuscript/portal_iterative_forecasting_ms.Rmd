---
title: "Automated iterative forecasting for the Portal Project"
csl: mee.csl
fontsize: 12pt
output:
  pdf_document:
    keep_tex: yes
  html_document: default
header-includes:
- \usepackage{times}
- \usepackage{setspace}
- \usepackage{booktabs}
- \doublespacing
- \usepackage{lineno}
- \linenumbers
geometry: left=3.5cm, right=3.5cm, top=2.25cm, bottom=2.25cm, headheight=12pt, letterpaper
bibliography: refs.bib
---
\raggedright


# Introduction

Forecasting the future state of ecological systems is important for management, conservation, and evaluation of our basic understanding of how ecology operates [@clark2001; @tallis2006; @diaz2015; @dietze2017]. In 2001, Clark et al. [-clark2001] called for a more central role of forecasting in ecology, and since then an increasing number of ecological forecasts are being published. However, most of these forecasts are made once, published, and never assessed or updated. Without assessment, we have limited information on how much confidence to place in our predictions; without regular updates, forecasts lack the most up-to-date information as conditions change [@dietze2018]. This lack of both regular assessment and active updating has limited the progress of ecological forecasting and hindered our ability to make useful and reliable predictions. More regular updating and assessment will advance ecological forecasting as a field by speeding up the identification of the best models for forecasting specific issues in specific ecosystems and contribute overall to our understanding of how to best design forecasting approaches for ecology. For ecological forecasting to mature as a field, we need to change how we produce and interact with forecasts, creating a more dynamic interplay between model development, prediction generation, and incorporation of new data and information [@dietze2018].

With the goal of making ecological forecasting more dynamic and responsive, Dietze et al [-dietze2018] recently called for an increase in iterative near-term forecasting. Iterative near-term forecasting means making forecasts for the near future and making these forecasts repeatedly through a cycle of forecast evaluation, integration of updated data, and generation of new forecasts. This approach to forecasting has a number of advantages. Because forecasts are made 'near-term'---daily to annual forecasting instead of multi-decadal---predictions can be assessed more quickly and frequently, leading to more rapid model improvements [@tredennick2016; @dietze2018]. Because the forecasts are made repeatedly through time, new data can be integrated with each new iteration of the forecast cycle. This iterative approach to forecasting allows any changes in the state of the system that have occurred since the previous forecast to be incorporated and accounted for, thereby improving the accuracy of future forecasts [@dietze2018]. Iterative near-term forecasting has the potential to promote rapid improvement in the state of ecological forecasting by quickly identifying how models are failing, facilitating rapid testing of improved models, and incorporating updated data so models run with the most up-to-date information on the system available. While use of iterative near-term forecasting is often discussed in the context of policy and management [@clark2001; @Luo2011; @petchey2015], this approach to model testing can also be used to improve our basic understanding of ecological systems [@dietze2017]. For example, alternative mechanistic models can be competed against each other to see which model provides the best forecasts for near-term dynamics, thus providing insights into the relative importance of different processes driving dynamics of ecological systems [@dietze2018]. Whether deployed for basic or applied uses, iterative near-term forecasting incorporates a more dynamic interplay between models, predictions, and data that is clearly needed to improve ecological forecasting and our understanding of ecological systems more broadly.

Because iterative near-term forecasting requires a dynamic interplay of models, predictions, and data, Dietze et al [-dietze2018] highlight approaches to data management, model construction and evaluation, and cyberinfrastructure that are necessary to effectively implement this type of forecasting (Box 1). Data to be used for iterative near-term forecasting needs to be widely accessible, which requires data to be released quickly under open licenses [@dietze2018; @vargas2017] and structured so that it can be used easily by a variety of researchers and in multiple modeling approaches [@borer2009; @strasser2011]. Models need to be able to deal with uncertainty, in the predictors and the predictions, to properly convey uncertainty in the resulting forecasts [@diniz-filho2009]. Multiple models should be developed, both to assess which models are performing best [@dietze2018] and to facilitate combining models to form ensemble predictions which tend to perform better than single models [@araujo2007; @diniz-filho2009]. Ensuring that data and models are regularly updated and new forecasts are made requires cyberinfrastructure to automate data processing, model fitting, prediction, model evaluation, forecast visualization, and archiving. In combination, these approaches should allow forecasts to be easily rerun and evaluated as new data becomes available [Box 1; @dietze2018].

While iterative near-term forecasting is an important next step in the evolution of ecological forecasting, the requirements outlined by Dietze et al (Box 1) are not trivial to implement; few of their recommendations are in widespread use in ecology today. We examined what it would entail to operationalize Dietze et al's recommendations by constructing our own iterative near-term forecasting pipeline for an on-going long-term (~40 year) ecological study that collects high-frequency data on desert rodent abundances [@brown1998; @ernest2008]. We constructed our forecasting pipeline with the goal of being able to forecast rodent abundances and evaluate our predictions on a monthly basis. In this paper, we discuss our approach for creating this iterative near-term forecasting pipeline, the challenges we encountered, the tools we used, and the lessons we learned so that others can create their own iterative forecasting systems.

# System Background

Iterative forecasting requires data that is collected repeatedly, and it benefits most from data that is collected frequently, as this provides more opportunities for updating model results and assessing (and potentially improving) model performance [Box 1; @dietze2018]. The Portal Project is a long-term ecological study situated in the Chihuahuan Desert (2 km north and 6.5 km east of Portal, Arizona, US). Researchers have been continuously collecting data at the site since 1977, including data on the abundance of rodent and plant species (monthly and twice yearly, respectively) and climactic factors such as air temperature and precipitation (daily)[@brown1998; @ernest2009; @ernest2016]. The site consists of 24 50m x 50m experimental plots. Each plot contains 49 permanently marked trapping stations laid out in a 7 x 7 grid, and all plots are trapped with Sherman live traps for one night each month. For all rodents caught during a trapping session, information on species identity, size, and reproductive condition is collected, and new individuals are given identification tags. This data on rodent populations is high-frequency, uses consistent trapping methodology, and has an extended time-series (469 monthly samples and counting), making this study an ideal case for near-term iterative forecasting. 

# Implementing an automated iterative forecasting system

Implementation of iterative forecasting requires the regular rebuilding of models with new raw data as it becomes available and the presentation of those forecasts in usable forms; in our case, this occurs monthly. Rebuilding models in an efficient and maintainable way relies on developing an automated pipeline to handle the six stages of converting raw data into new forecasts: data collection, data sharing, data manipulation, modeling and forecasting, archiving, and presenting of the forecasts (Figure 1). To implement the pipeline outlined in Figure 1, we used a "continuous analysis" framework [*sensu* @beaulieu2017] that automatically processes the most up-to-date data, refits the models, makes new forecasts, archives the forecasts, and updates a website with analysis of current and previous forecasts. In this section we describe our approach to streamlining and automating the multiple components of the forecasting pipeline and the tools and infrastructure we employed to execute each stage of the pipeline.

![Figure 1. Stages of the forecasting pipeline. To go from raw data to forecast presentation involves a number of stages, each of which requires unique tasks, tools and infrastructure. While each stage has unique tasks associated with it, the stages of our pipeline are also interdependent. Our pipeline is primarily a linear structure where outputs from one stage form the inputs for the subsequent stage. All stages require code written in R. ](figs/pipeline_fig.png).

## Continuous Analysis Framework

A core aspect of iterative near-term forecasting is the regular rerunning of the forecasting pipeline. We chose to have the computer run our pipeline and employed "continuous analysis" [*sensu* @beaulieu2017] to drive the automation of both the full pipeline and a number of its individual components. Continuous analysis uses a set of tools originally designed for software development called "continuous integration" (CI). CI combines computing environments for running code with monitoring systems to identify changes in data or code. Essentially, CI is a computer helper whose job is to watch the pipeline and, when it sees a change in the code or data, it runs all the computer scripts needed to ensure that the forecasting pipeline runs from beginning to end. This is useful for iterative near-term forecasting because it does not rely on humans to remember to create forecasts when new models or data are added. These tools are common in the area of software development where they are used to automate software testing and integrate work by multiple developers working on the same code base. However, these tools can be used for any computational task that needs to be regularly repeated or run after changes to code or data [@beaulieu2017]. Our forecasting pipeline currently runs on a publicly available continuous integration service (Travis CI; https://travis-ci.org/) that is free for open source projects (up to a limited amount of computing time). Because of the widespread use of CI in software development, alternative services that can run code on local or cloud-based computational infrastructure also exist (e.g., Drone (http://try.drone.io/)) [@beaulieu2017]. As detailed below, we use CI to quality check data, test code using "unit tests" [@wilson2014], build models, make forecasts, and publicly present and archive the results (Figure 2).

![Figure 2. Continuous integration system. Each box denotes the core infrastructure used for each stage of the forecasting pipeline denoted in Figure 1. Continuous integration, denoted here with the Travis icon (a girl wearing safety glasses and hardhat), plugs into our pipeline at every stage.  Travis triggers the code involved in major events that involve integration across stages of the pipeline, such as taking the output from the forecasting stage (purple box) to create an updated presentation (rose box). Travis also runs tasks within a stage, such as conducting tests to make sure code changes have not introduced errors (icons on arrows originating and ending on the same box)](figs/contanaly_fig.png)

In addition to automatically running software pipelines, the other key component of "continuous analysis" is making sure that the pipelines will continue to run even as software dependencies change [@beaulieu2017]. Many of us have experienced the frustrations that can occur when software updates (e.g., changes in R package versions) create errors in previously functional code. We experienced this issue when a package one of our models relies on, `tscount` [@liboschik2015], was temporarily removed from CRAN (the R package repository) and, therefore, could not be installed in the usual way. This broke our forecasting pipeline because we could no longer run models that used that package. To make our pipeline robust to changes in external software dependencies, we follow Beaulieu and Greene's [-@beaulieu2017] recommendation to use software containers. Software containers are standalone packages that contain copies of everything you need to run some piece of software, including the operating system. Once created, a software container is basically a time capsule, it contains all the software dependencies in the exact state used to develop and run the software. If those dependencies change (or disappear) in the wider world, they still exist, unchanged, in your container. We use an existing platform, Docker [@Merkel2014], to store an exact image of the complete software environment for running the forecasts. Docker also allows a specified set of packages to be used consistently across different computer and server environments. Using containers allows us to update to new package versions after testing and making any necessary changes to the data processing and analysis code. We use a container created by the Rocker project which is a Docker image with many important R packages (i.e. tidyverse) pre-installed  [@boettiger2017]. We add our code and dependencies to this existing Rocker image to create a software container for our forecasting pipeline. In combination, the automated running of the pipeline (continuous integration) and the guarantee it will not stop working unexpectedly due to software dependencies (via a software container) allows continuous analysis to serve as the glue that connects all stages of the forecasting pipeline.

## Data Collection, Entry, and Processing

Iterative forecasting benefits from frequently updated data so that state changes can be quickly incorporated into new forecasts [@dietze2018]. Frequent data collection and rapid processing are both important for providing timely forecasts. Since our data is collected monthly, ensuring that the models have access to the newest data requires a data latency period of less than 1 month from collection to availability for modeling. To accomplish this, we automated components of the data processing and quality assurance/quality control (QA/QC) process to reduce the time needed to add new data to the database (Figures 1 and 2). 

New data is double-entered into Microsoft Excel using the "data validation" feature. The two versions are then compared in an R script to control for errors in data entry. Quality control (QC) checks using the `testthat` R package [@wickham2011] are run on the data to test for validity and consistency both within the new data and between the new and archived data. The local use of the QC scripts to flag problematic data greatly reduces the time spent error-checking and ensures that the quality of data is consistent. The data is then uploaded to the GitHub-based Portal Data repository. GitHub (https://github.com/) is a software development tool for managing computer code development, but we have also found it useful for data management. On GitHub, changes to data can be tracked through the Git version control system which logs all changes made to any files in the repository. All updates to data are processed through "pull requests", which are notifications that someone has a modified version of the data to contribute. QA/QC checks are automatically rerun on the submitted data using continuous integration to ensure that these checks have been run and that no avoidable errors reach the official version of the dataset.

We also automated the updating of supplementary data tables, including information on weather and trapping history, that were previously updated manually. As soon as new field data is merged into the repository, continuous integration updates all supplementary files. Weather data is automatically fetched from our cellular-connected weather station, cleaned, and appended to the weather data table. Supplementary data tables related to trapping history are updated based on the data added to the main data tables. Using CI for this ensures that all supplementary data tables are always up-to-date with the core data.

## Data Sharing

The Portal Project has a long history of making its data publicly available, which means that anyone can use it for forecasting or other projects. Historically the publication of the data was conducted through data papers [@ernest2009,@ernest2016], the most common approach in ecology; however, this approach caused years of data latency. Recently, the project has switched to posting data directly to a public GitHub repository with a CC0 (i.e. no rights reserved) license (Figure 1). This immediate posting reduces that data latency to less than one month and, therefore, makes meaningful iterative near-term forecasting possible for not only our group but other interested parties, as well.

## Data Manipulation

Once data is available, it needs to be processed into a form appropriate for modeling (Figure 1). For many ecological datasets, this requires not only simple data manipulation but also a good understanding of the data to allow data to facilitate appropriate aggregation. Data manipulation steps are often conducted using custom one-off code to convert the raw data into the desired form [@morris2013], but this approach has several limitations. First, each researcher must develop and maintain their own data manipulation code, which is inefficient and can result in different researchers producing different versions of the data for the same task. Subtle differences in data processing decisions have led to confusion when reproducing results for the Portal data in the past. Second, this kind of code is rarely robust to changes in data structure and location. Based on our experience developing and maintaining the Data Retriever [@morris2013; @senyondo2017], these kinds of changes are common. Finally, this kind of code is generally poorly tested, which can lead to errors based on mistakes in data manipulation. To avoid these issues for the Portal Project data, the Portal team has been developing an R package (portalR; http://github.com/weecology/portalr) for acquiring the data and handling common data cleaning and aggregation tasks. As a result, our modeling and forecasting code only needs to install this package and run the data manipulation and summary functions to get the appropriate data (Figure 2). The package undergoes thorough automated unit tests to ensure that data manipulations are achieving the desired results. Having data manipulation code maintained in a separate package that focuses on consistently providing properly summarized forms of the most recent data has made maintaining the forecasting code itself much more straightforward.

## Modeling and Forecasting

Iterative near-term forecasting involves regularly refitting a variety of different models (Figure 1). Additionally, any new models should be easy to incorporate to allow for iterative improvements to the general modeling structure and approach. We use CI to refit the models and make new forecasts each time the modeling code changes and when new data becomes available (Figure 2). We use a plugin infrastructure to allow new models to be easily added to the system. Our approach is to treat each model as interchangable black boxes - we feed them all the same inputs and we expect them to generate the same structure for model outputs (Figure 3). Each iteration of the forecasting cycle, all models are run and the standardized outputs are combined into a single file to store the results of the different models' forecasts. This structure allows weighted ensemble models to be produced based on how well individual models fit the training data. Our hope is that by requiring all models to take the same input (though models may differ in how they use that input) and provide the same outputs, we've gained the capacity to add and compare very different types of models that we may not currently be able to foresee and design around. Our infrastrcture should allow easy addition of more complicated models and more sophisticated approaches to creating ensembles even though to date our models and ensemble-creating approaches are extremely basic.

![Figure 3. Demonstration of plugin infrastructure. All model scripts (represented here by the example AR1.R) are housed in a single folder. Each model script uses data provided by the core forecasting code (represented here by rodent.csv) and returns its forecast outputs in a predefined structure that is consistent across models (represented here by the example 2017_12_08forecasts.csv). Outputs from all models run on a particular date are combined into the same file (i.e. 2017_12_08forecasts.csv) to allow cross-model evaluations. Model outpus files are housed in a folder containing all forecast outputs from all previous dates to facilitate our arhciving process and and forecast assessment activities ](figs/plugin_fig.gif)

We designed our forecast output format to to store relatively generic forecasts information to allow flexibility in what the models predict (i.e. species abundances, biomass). Each forecast output file contains the date being forecast, the collection date of the data used for fitting the models, the date the forecast was made, the state variable being forecast (e.g., rodent biomass, the abundance of a species), and the forecast value and associated uncertainty of that forecast (Figure 3). Our goal is to store a variety of different forecasts in a common format with the hope of this being a useful starting point for developing a standard for storing ecological forecasts more generally.

Our forecasts are currently evaluated using root mean square error (RMSE) to evaluate point forecasts and coverage to evaluate uncertainty. We plan to add additional metrics in the future. In addition to evaluating the actual forecasts, we also use hindcasting (forecasting on already collected data) to gain additional insight into the methods that work best for forecasting this system. For example, a model is fit using rodent observations up to June 2005, then used to make a forecast 12 months out to May 2006. The observations of that 12 month period can immediately be used to evaluate the model. Hindcasting can be conducted using data that was already been collected, thus allowing model comparison of large numbers of hindcasts and giving insight into which models make the best forecasts without needing to wait for new data to be collected. It can also be used to quickly evaluate new models instead of waiting for an adequate amount of data to accumulate.

## Archiving

Publicly archiving forecasts before new data is collected allows the field to assess, compare, and build on forecasts made by different groups [@mcgill2012; @tredennick2016; @dietze2017; @harris2018] (Figure 1). Archiving serves as a form of pre-registration for model predictions, helping facilitate unbiased interpretation of model performance. We wanted our archives to not only be accessible to others but to also be a permanent record whose continued existence was not dependent on any particular researcher or funding stream. Storing forecast outputs in our forecasting GitHub repository was not sufficient for archival purposes because repositories can be deleted.  We explored three major repositories for archiving our forecasts: FigShare (https://figshare.com/), Zenodo (https://zenodo.org/), and Open Science Framework (https://osf.io/). While all three repositories allowed for easy manual submissions (i.e., a human uploading files after each forecast), automating this process was substantially more difficult. Various combinations of repositories, APIs (i.e., interfaces that allow automated ways interacting with these websites) and associated R packages had issues with: 1) integrating authorization with continuous integration; 2) automatically making archived files public; 3) adding new files to an existing location; or 4) automatically permanently archiving the files. Our eventual solution was to leverage the GitHub-Zenodo integration (https://guides.github.com/activities/citable-code/) and automatically push forecasts to the GitHub repository from the CI server. The GitHub-Zenodo integration is designed to automatically create versioned archives of GitHub repositories. There is an existing one-time process for linking our forecasting repository on  GitHub with Zenodo. Once this link is created, each time a new forecast is created, our pipeline adds the new forecasts to the GitHub repository and uses the GitHub API to create a new "release" of our repository. This triggers the Zenodo-GitHub integration, which automatically archives the resulting forecasts and the code that generated them under a top-level DOI that refers to all archived forecasts ([https://doi.org/10.5281/zenodo.833438](https://doi.org/10.5281/zenodo.833438)). Through this process, we automatically archive every forecast made with a documented history of the archive. While this approach is functional because everything in the repository is archived when a new forecast is made, these archives have complicated folder structures, making it more harder for users to find and access the forecasting results. Finding simpler approaches to automated archiving would increase the transparency of the forecase results and their ease of use by external researchers.

## Presentation

In addition to archiving the results of each forecast, we present them on a website that displays monthly rodent forecasts, model evaluation metrics, monthly reports, and information about the study site (Figure 4; [http://portal.naturecast.org](http://portal.naturecast.org/)). The website includes a graphical presentation of the most recent month's forecasts (including uncertainty) and compares the latest data to the previous forecasts. Information on the the species and the field site targeted to a general audience are also included. The site is built using Rmarkdown [@rmarkdown], which naturally integrates into the pipeline, and is automatically updated after each forecast. The `knitr` R package [@xie2015] compiles the code into HTML, which is then published using Github Pages (https://pages.github.com/). The files for the website are stored in a subdirectory of the forecasting repository. As a result, the website is also archived automatically as part of the forecast archiving.

![Figure 4. Screen capture of the homepage of the Portal Forecasting website; http://portal.naturecast.org. This site contains information on the most current forecasts, evaluation of forecast performance, and general information about the species being forecast.](figs/website_fig.png)

# Discussion

Following the recommendations of Dietze et al [-dietze2018], we developed an automated iterative forecasting system (Figures 1 and 2) to support repeated forecasting of an ecological system. Our forecasting system automatically acquires and processes the newest data, refits the models, makes new forecasts, publicly archives those forecasts, and presents both the current forecast and information on how previous forecasts performed. Every week our forecasting system generates a new set of forecasts with no human intervention, except for the entry of new field data. This ensures that forecasts based on the most recent data are always available and allows us to rapidly assess the performance of multiple forecasting models for a number of different states of the system, including the abundances of individual species and community-level variables such as total abundance. To create this iterative near-term forecasting system, we used R to process data and conduct analyses, and we leveraged already existing services (i.e. GitHub, Travis, Docker) for more complicated cyberinfrastructure tasks. Thus, our approach to developing iterative near-term forecasting infrastructure provides an example for how short-term ecological forecasting systems can be initially developed. 

We designed this forecasting system with the goal of making it relatively easy to build, maintain, and extend. We used existing technology for both running the pipeline and building individual components, which allowed is to build the system relatively cheaply in terms of both time and money. This included the use of tools like Docker for reproducibility, the Travis CI continuous integration system for automatically running the pipeline, Rmarkdown and `knitr` for generating the website, and the already existing integration between Github and Zenodo  to archive the forecasts. By using this "continuous analysis" approach [@beaulieu2017], where analyses are automatically rerun when changes are made to data, models, or associated code, we have reduced the time required by scientists to run and maintain the forecasting pipeline. To make the system extensible so that new models could be easily incorporated, we used a plugin-based infrastructure so that adding a new model to the system is as easy as adding a single file to the 'models' folder in our repository (Figure 3). This should substantially lower the barriers to other scientists contributing models to this forecasting effort. We also automatically archive the resulting forecasts publicly so thatthe performance of these forecasts can be assessed by both us and other researchers as new data is collected. This serves as a form of pre-registration of expected results () by providing a quantitative record of the forecast before the data being predicted were collected. 

While building this system was facilitated by the use of existing technological solutions, there were still a number of challenges in making existing tools work for automated iterative forecasting. Continuous integration is designed primarily for running automated tests on software, not for running a coordinated forecasting pipeline. As a result, extra effort was sometimes necessary to figure out how to get these systems to work properly in non-standard situations, like running code that was not part of a software package. In addition, hosted continuous integration solutions, like Travis, provide only limited computational resources. As the number and complexity of the models we fit has grown, we have had to continually invest effort in reducing our total compute time so we can stay within these limits. Finally, we found no satisfactory existing solution for archiving our results. All approaches we tried had limitations when it came to automatically generating publicly versioned archives of forecasts on a repeated basis. Overall, we found existing technology to be sufficient to the task, but it required greater expertise and a greater investment of time than is ideal. Tool development to reduce the effort required for scientists to set up their own short-term forecasting systems would clearly be useful, but our efforts show that it is possible for scientists to use existing tools to develop initial iterative systems as a method for both advancing scientific understanding and developing proof of concept forecasting systems. Forecasting systems such as ours can begin the process of making iterative forecasts while we wait for more integrated and easier to deploy platforms for iterative forecasting to be developed. Once more integrated platforms exist, it will be easier to convert current systems into fully operationalized forecasting systems that are relied on for decision making [@dietze2018, other paper by operationalization co-authors].

Because of the breadth of expertise needed to set up our forecasting pipeline, our effort required a team with diverse skills and perspectives, ranging from software development to field site expertise. It is rare to find such breadth within a single research group, and our system was developed as a collaboration between the lab collecting the data and a computational ecology lab. When teams have a breadth of expertise, communication can be challenging. We found a shared base of knowledge related to both the field research and fundamental computational skills was important for the success of the group. Everyone on the team had received training in fundamental data management and computing skills through a combination of university courses, Software and Data Carpentry workshops [@teal2015], and informal lab training efforts. In addition, everyone was broadly familiar with the study site and methods of data collection, and most team members had participated in field work at the site on multiple occasions. This provided a shared set of knowledge and vocabulary that actively facilitated interdisciplinary interactions. Given the current state of tools for forecasting, forecasting teams will need people with significant experience in working with continuous integration and APIs, which means interdisciplinary teams will generally be required for creating these pipelines until tool development improves. 

We developed this infrastructure for automatically making iterative forecasts with the goals of making accurate forecasts for this well-studied system, learning what methods work well for ecological forecasting more generally, and improving our understanding of the processes driving ecological dynamics. The most obvious application of automated iterative ecological forecasting is for speeding up development of forecasting models by providing the most recent data available to models and by quickly iterating to improve the models used for forecasting. By learning what works best for forecasting in this and other ecological systems, we will better understand what the best approaches are for ecological forecasting more generally. By designing the pipeline so that it can forecast many different aspects of the ecological community, we also hope to learn about what aspects of ecology are more forecastable. Finally, automated forecasting infrastructures like this one also provide a core foundation for faster scientific inquiry more broadly because new models can quickly be applied to data and compared to existing models. The forecasting infrastructure does the time-consuming work of data processing, data integration, and model assessment, allowing new research to focus on the models being developed and the inferences about the system that can be drawn from them [@dietze2018]. We plan to use this pipeline to drive future research into understanding the processes that govern the dynamics of individual populations and the community as a whole at this long-term study site. By regularly running different models for population and community dynamics, a near-term iterative pipeline such as ours should also make it possible to rapidly detect changes in how the system is operating, which should allow the rapid identification of ecological transitions or even possibly allow them to be prevented [@carpenter example]. By building an automated iterative near-term forecasting infrastructure we can improve our ability to forecast natural systems, our understanding of the biology driving ecological dynamics, and detect or even predict changes in system state that are important for conservation and management.


# Acknowledgements

This research was supported by the National Science Foundation through grant 1622425 to S.K.M. Ernest and by the Gordon and Betty Moore Foundation’s Data-Driven Discovery Initiative through grant GBMF4563 to E.P. White. We thank all of the graduate students, postdocs, and volunteers who have collected the Portal Project over the last 40 years and the developers of all of the software and tools that made this project possible.

# Box 1. Key practices for automated iterative near-term ecological forecasting

A list of some of the key practices developed by Dietze et al [-dietze2018] for facilitating iterative near-term ecological forecasting and discussion of why these practices are important.

## Data

**1. Frequent data collection**

Frequent data collection allows models to be regularly updated and forecasts to be frequently evaluated [@dietze2018]. Depending on the system being studied, this frequency could range from sub-daily to annual, but typically the more frequently the data is collected the better.

**2. Rapid data release under open licenses**

Data should be released as quickly as possible (low latency) under open licenses so that forecasts can be made frequently and data can be accessed by a community of forecasters [@dietze2018; @vargas2017].

**3. Best practices in data structure**

To reduce the time and effort needed to incorporate this data into models, best practices in data structure need to be employed for managing and storing collected data to ensure it is easy to integrate into other systems (interoperability) [@borer2009; @strasser2011; @white2013].

## Models

**4. Focus on uncertainty**

Understanding the uncertainty of forecasts is crucial to interpreting the forecasts and understanding their utility. Models used for forecasting should be probabilistic to properly quantify uncertainty and to convey how this uncertainty increases through time. Evaluation of forecast models should include assessment of how accurately they quantify uncertainty as well as point estimates. This can be done using "proper and local" scores [@hooten2015].

**5. Compare forecasts to simple baselines**

Understanding how much information is present in a forecast requires comparing its accuracy to simple baselines to see if the models yield improvements over the naive expectation that the system is static [@harris2018].
 
**6. Compare and combine multiple modeling approaches**
 
To quickly learn about the best approaches to forecasting different aspects of ecology, multiple modeling approaches should be compared for forecasting tasks [@harris2018]. Different modeling approaches should also be combined into ensemble models, which are known to outperform single models for many forecasting and prediction tasks [@weigel2008].

## Cyberinfrastructure

In addition to improvements in data and models, iterative near-term forecasting requires improved infrastructure and approaches to support continuous model development and iterative forecasting [@dietze2018].

**7. Best practices in software development**

Best practices should be followed in the development of scientific software and modeling to make it easier to maintain, integrate into pipelines, and build on by other researchers. Key best practices include using open licenses, good documentation, version control, and cross-platform support [@wilson2014; @hampton2015].

**8. Support easy inclusion of new models**

To facilitate the comparison and ensembling of different modeling approaches, code for fitting models and making forecasts should be easily extensible, allowing models developed by different groups to be easily integrated into a single framework [@dietze2018].

**9. Automated end-to-end reproducibility**

Iteratively making forecasts requires that acquiring the newest data, refitting the models, and making new forecasts is simple. Ideally, this should be done automatically without requiring human intervention. Therefore, the process of making forecasts should emphasize end-to-end reproducibility, including data, models, and evaluation [@stodden2014], to allow the forecasts to be easily rerun as new data becomes available [@dietze2018]. Ideally, the entire forecasting pipeline will be rerun automatically as new data becomes available.

**10. Publicly archive forecasts**

Forecasts should be openly archived to demonstrate that the forecasts were made without knowledge of the outcomes (i.e., as a form of pre-registration *sensu*) and to allow the community to assess and compare the performance of different forecasting approaches both now and in the future [@mcgill2012; @tredennick2016; @dietze2018; @harris2018]. Ideally, the forecasts and evaluation of their performance should be automatically posted publicly in a manner that is understandable by both interested scientists and other stakeholders.


# References
